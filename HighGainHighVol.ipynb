{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRSO1w2UsE5GokMHPU8dMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanozzz/First_app/blob/master/HighGainHighVol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oixi1-WAdddt",
        "outputId": "810392f3-9193-4ff4-8836-8085c7b8cf0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 16:53:57 - INFO - Logger initialized successfully with IST timezone.\n",
            "INFO:HighGainHighVolLogger:Logger initialized successfully with IST timezone.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File logging initialized at: /content/HighGainHighVol_2025-01-02.log\n",
            "Console logging initialized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-01-02 16:54:00 - INFO - OpenAI client initialized successfully.\n",
            "INFO:HighGainHighVolLogger:OpenAI client initialized successfully.\n",
            "2025-01-02 16:54:00 - INFO - TopLosers class initialized successfully.\n",
            "INFO:HighGainHighVolLogger:TopLosers class initialized successfully.\n",
            "2025-01-02 16:54:00 - INFO - Input file located at: /content/HighGainHighVol.csv\n",
            "INFO:HighGainHighVolLogger:Input file located at: /content/HighGainHighVol.csv\n",
            "2025-01-02 16:54:00 - INFO - Starting the Top Gainers and Losers process.\n",
            "INFO:HighGainHighVolLogger:Starting the Top Gainers and Losers process.\n",
            "2025-01-02 16:54:00 - INFO - Selecting top 4 stocks by DailyTurnInCr.\n",
            "INFO:HighGainHighVolLogger:Selecting top 4 stocks by DailyTurnInCr.\n",
            "2025-01-02 16:54:00 - INFO - Generating Contextual summaries.\n",
            "INFO:HighGainHighVolLogger:Generating Contextual summaries.\n",
            "2025-01-02 16:54:17 - INFO - Perplexity summary generated for Bajaj Finance.\n",
            "INFO:HighGainHighVolLogger:Perplexity summary generated for Bajaj Finance.\n",
            "2025-01-02 16:54:34 - INFO - Perplexity summary generated for Eicher Motors.\n",
            "INFO:HighGainHighVolLogger:Perplexity summary generated for Eicher Motors.\n",
            "2025-01-02 16:54:51 - INFO - Perplexity summary generated for Bajaj Finserv.\n",
            "INFO:HighGainHighVolLogger:Perplexity summary generated for Bajaj Finserv.\n",
            "2025-01-02 16:55:10 - INFO - Perplexity summary generated for CreditAccess Grameen.\n",
            "INFO:HighGainHighVolLogger:Perplexity summary generated for CreditAccess Grameen.\n",
            "2025-01-02 16:55:10 - INFO - Fetching news from Perplexity.\n",
            "INFO:HighGainHighVolLogger:Fetching news from Perplexity.\n",
            "2025-01-02 16:55:24 - INFO - Perplexity API response received for query: Find recent news and updates about Bajaj Finserv in the last week.\n",
            "INFO:HighGainHighVolLogger:Perplexity API response received for query: Find recent news and updates about Bajaj Finserv in the last week.\n",
            "2025-01-02 16:55:25 - INFO - Perplexity API response received for query: Find recent news and updates about CreditAccess Grameen in the last week.\n",
            "INFO:HighGainHighVolLogger:Perplexity API response received for query: Find recent news and updates about CreditAccess Grameen in the last week.\n",
            "2025-01-02 16:55:32 - INFO - Perplexity API response received for query: Find recent news and updates about Eicher Motors in the last week.\n",
            "INFO:HighGainHighVolLogger:Perplexity API response received for query: Find recent news and updates about Eicher Motors in the last week.\n",
            "2025-01-02 16:55:34 - INFO - Perplexity API response received for query: Find recent news and updates about Bajaj Finance in the last week.\n",
            "INFO:HighGainHighVolLogger:Perplexity API response received for query: Find recent news and updates about Bajaj Finance in the last week.\n",
            "2025-01-02 16:55:34 - INFO - Combining Contextual summaries and Perplexity news.\n",
            "INFO:HighGainHighVolLogger:Combining Contextual summaries and Perplexity news.\n",
            "2025-01-02 16:56:48 - INFO - Generating final news byte for all stocks.\n",
            "INFO:HighGainHighVolLogger:Generating final news byte for all stocks.\n",
            "2025-01-02 16:56:48 - INFO - Starting to generate the initial news byte.\n",
            "INFO:HighGainHighVolLogger:Starting to generate the initial news byte.\n",
            "2025-01-02 16:57:20 - INFO - Initial news byte generated successfully.\n",
            "INFO:HighGainHighVolLogger:Initial news byte generated successfully.\n",
            "2025-01-02 16:57:20 - INFO - Cleaned citations from the initial news byte.\n",
            "INFO:HighGainHighVolLogger:Cleaned citations from the initial news byte.\n",
            "2025-01-02 16:57:20 - INFO - Initial news byte generated with 341 words after cleaning.\n",
            "INFO:HighGainHighVolLogger:Initial news byte generated with 341 words after cleaning.\n",
            "2025-01-02 16:57:20 - INFO - Initial news byte is within the word limit. Returning the final news byte.\n",
            "INFO:HighGainHighVolLogger:Initial news byte is within the word limit. Returning the final news byte.\n",
            "2025-01-02 16:57:20 - INFO - Refining the FinalNewsByte.\n",
            "INFO:HighGainHighVolLogger:Refining the FinalNewsByte.\n",
            "2025-01-02 16:57:37 - INFO - News byte refined successfully.\n",
            "INFO:HighGainHighVolLogger:News byte refined successfully.\n",
            "2025-01-02 16:57:37 - INFO - Generating a 1-minute ReelByte for individual stocks and combined news.\n",
            "INFO:HighGainHighVolLogger:Generating a 1-minute ReelByte for individual stocks and combined news.\n",
            "2025-01-02 16:57:37 - INFO - Generating 1-minute ReelBytes for high gain and high volume stocks.\n",
            "INFO:HighGainHighVolLogger:Generating 1-minute ReelBytes for high gain and high volume stocks.\n",
            "2025-01-02 16:57:37 - INFO - Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "INFO:HighGainHighVolLogger:Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "2025-01-02 16:57:58 - INFO - Reel summary successfully shortened.\n",
            "INFO:HighGainHighVolLogger:Reel summary successfully shortened.\n",
            "2025-01-02 16:57:58 - INFO - Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "INFO:HighGainHighVolLogger:Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "2025-01-02 16:58:18 - INFO - Reel summary successfully shortened.\n",
            "INFO:HighGainHighVolLogger:Reel summary successfully shortened.\n",
            "2025-01-02 16:58:18 - INFO - Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "INFO:HighGainHighVolLogger:Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "2025-01-02 16:58:34 - INFO - Reel summary successfully shortened.\n",
            "INFO:HighGainHighVolLogger:Reel summary successfully shortened.\n",
            "2025-01-02 16:58:34 - INFO - Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "INFO:HighGainHighVolLogger:Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "2025-01-02 16:58:50 - INFO - Reel summary successfully shortened.\n",
            "INFO:HighGainHighVolLogger:Reel summary successfully shortened.\n",
            "2025-01-02 16:58:50 - INFO - Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "INFO:HighGainHighVolLogger:Shortening summary for 1-minute reel focused on high gain and high volume counters.\n",
            "2025-01-02 16:59:09 - INFO - Reel summary successfully shortened.\n",
            "INFO:HighGainHighVolLogger:Reel summary successfully shortened.\n",
            "2025-01-02 16:59:09 - INFO - 1-minute ReelBytes successfully generated.\n",
            "INFO:HighGainHighVolLogger:1-minute ReelBytes successfully generated.\n",
            "2025-01-02 16:59:09 - INFO - ReelByte generated successfully.\n",
            "INFO:HighGainHighVolLogger:ReelByte generated successfully.\n",
            "2025-01-02 16:59:09 - INFO - Generating and saving plot.\n",
            "INFO:HighGainHighVolLogger:Generating and saving plot.\n",
            "<ipython-input-2-a96a2608f325>:949: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  bars = sns.barplot(\n",
            "2025-01-02 16:59:09 - INFO - Mobile-friendly horizontal bar plot saved to /content/HighGainHighVol.png\n",
            "INFO:HighGainHighVolLogger:Mobile-friendly horizontal bar plot saved to /content/HighGainHighVol.png\n",
            "2025-01-02 16:59:09 - INFO - Uploading the plot image to S3.\n",
            "INFO:HighGainHighVolLogger:Uploading the plot image to S3.\n",
            "2025-01-02 16:59:14 - INFO - Image uploaded to S3 successfully. URL: https://finbytes.s3.ap-south-1.amazonaws.com/images/highgainhighvolumetoday_2025-01-02_11-29-12.png\n",
            "INFO:HighGainHighVolLogger:Image uploaded to S3 successfully. URL: https://finbytes.s3.ap-south-1.amazonaws.com/images/highgainhighvolumetoday_2025-01-02_11-29-12.png\n",
            "2025-01-02 16:59:14 - INFO - Plot image successfully uploaded to S3. URL: https://finbytes.s3.ap-south-1.amazonaws.com/images/highgainhighvolumetoday_2025-01-02_11-29-12.png\n",
            "INFO:HighGainHighVolLogger:Plot image successfully uploaded to S3. URL: https://finbytes.s3.ap-south-1.amazonaws.com/images/highgainhighvolumetoday_2025-01-02_11-29-12.png\n",
            "2025-01-02 16:59:14 - INFO - Saving JSON summary locally.\n",
            "INFO:HighGainHighVolLogger:Saving JSON summary locally.\n",
            "2025-01-02 16:59:14 - INFO - JSON summary saved to /content/HighGainHighVolume.json\n",
            "INFO:HighGainHighVolLogger:JSON summary saved to /content/HighGainHighVolume.json\n",
            "2025-01-02 16:59:14 - INFO - Saving the final output CSV.\n",
            "INFO:HighGainHighVolLogger:Saving the final output CSV.\n",
            "2025-01-02 16:59:14 - INFO - Output saved to /content/HighGainHighVol_with_DetailedSummaries.csv\n",
            "INFO:HighGainHighVolLogger:Output saved to /content/HighGainHighVol_with_DetailedSummaries.csv\n",
            "2025-01-02 16:59:14 - INFO - Posting JSON data to API.\n",
            "INFO:HighGainHighVolLogger:Posting JSON data to API.\n",
            "2025-01-02 16:59:45 - ERROR - Error posting JSON to API: 503 Server Error: Service Unavailable for url: https://sy6foef31c.execute-api.ap-south-1.amazonaws.com/default/create\n",
            "ERROR:HighGainHighVolLogger:Error posting JSON to API: 503 Server Error: Service Unavailable for url: https://sy6foef31c.execute-api.ap-south-1.amazonaws.com/default/create\n",
            "2025-01-02 16:59:45 - ERROR - Error posting JSON data to API: 503 Server Error: Service Unavailable for url: https://sy6foef31c.execute-api.ap-south-1.amazonaws.com/default/create\n",
            "ERROR:HighGainHighVolLogger:Error posting JSON data to API: 503 Server Error: Service Unavailable for url: https://sy6foef31c.execute-api.ap-south-1.amazonaws.com/default/create\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import boto3\n",
        "import requests\n",
        "from textwrap import fill\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import pytz\n",
        "import openai\n",
        "import time\n",
        "\n",
        "\n",
        "# Allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set up the logger globally\n",
        "\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"Sets up a logger with both file and console handlers, formatted with IST timezone.\"\"\"\n",
        "    logger = logging.getLogger(\"HighGainHighVolLogger\")\n",
        "\n",
        "    # Avoid adding duplicate handlers\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    # Set the logging level\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    # Define a custom formatter with IST timezone\n",
        "    class ISTFormatter(logging.Formatter):\n",
        "        def formatTime(self, record, datefmt=None):\n",
        "            ist = timezone(\"Asia/Kolkata\")\n",
        "            record_time = datetime.fromtimestamp(record.created, ist)\n",
        "            return record_time.strftime(datefmt or \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Create the formatter\n",
        "    formatter = ISTFormatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "    # File handler setup\n",
        "    try:\n",
        "        log_file = f\"/content/HighGainHighVol_{datetime.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d')}.log\"\n",
        "        file_handler = logging.FileHandler(log_file, mode=\"a\")  # Append mode\n",
        "        file_handler.setLevel(logging.INFO)\n",
        "        file_handler.setFormatter(formatter)\n",
        "        logger.addHandler(file_handler)\n",
        "        print(f\"File logging initialized at: {log_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to set up file handler: {e}\")\n",
        "\n",
        "    # Stream handler setup (console logging)\n",
        "    try:\n",
        "        console_handler = logging.StreamHandler()\n",
        "        console_handler.setLevel(logging.INFO)\n",
        "        console_handler.setFormatter(formatter)\n",
        "        logger.addHandler(console_handler)\n",
        "        print(\"Console logging initialized.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to set up console handler: {e}\")\n",
        "\n",
        "    # Test the logger\n",
        "    logger.info(\"Logger initialized successfully with IST timezone.\")\n",
        "    return logger\n",
        "\n",
        "\n",
        "# Initialize the logger\n",
        "logger = setup_logger()\n",
        "\n",
        "\n",
        "\n",
        "def get_ist_timestamp():\n",
        "    \"\"\"Returns the current timestamp in IST.\"\"\"\n",
        "    ist = pytz.timezone('Asia/Kolkata')\n",
        "    return datetime.now(ist).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "class HighGainHighVol:\n",
        "    def __init__(self, input_path='/content/HighGainHighVol.csv', output_path='/content/HighGainHighVol_with_DetailedSummaries.csv', plot_path='/content/HighGainHighVol.png'):\n",
        "        \"\"\"\n",
        "        Initialize the Losers class with file paths and API keys for OpenAI and Perplexity.\n",
        "        \"\"\"\n",
        "        self.input_path = input_path\n",
        "        self.output_path = output_path\n",
        "        self.plot_path = plot_path\n",
        "\n",
        "        # Fetch API keys from Colab's userdata or environment variables\n",
        "        openai_api_key = (userdata.get(\"OPENAI_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\")).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
        "        perplexity_api_key = (userdata.get(\"PERPLEXITY_API_KEY\") or os.environ.get(\"PERPLEXITY_API_KEY\")).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "\n",
        "        if not openai_api_key:\n",
        "            logger.error(\"OpenAI API key is not set. Please configure it in userdata or environment variables.\")\n",
        "            raise ValueError(\"OpenAI API key is missing.\")\n",
        "        if not perplexity_api_key:\n",
        "            logger.error(\"Perplexity API key is not set. Please configure it in userdata or environment variables.\")\n",
        "            raise ValueError(\"Perplexity API key is missing.\")\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        self.openai_api_key = openai_api_key\n",
        "        self.initialize_openai_client()\n",
        "\n",
        "        # Store the Perplexity API key for later use\n",
        "        self.perplexity_api_key = perplexity_api_key\n",
        "        self.api_url = \"https://api.perplexity.ai/chat/completions\"\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "\n",
        "        # Log successful initialization\n",
        "        logger.info(\"TopLosers class initialized successfully.\")\n",
        "\n",
        "        # Validate the input file path\n",
        "        self.validate_input_path()\n",
        "\n",
        "    def initialize_openai_client(self):\n",
        "        \"\"\"Initialize the OpenAI client with the API key.\"\"\"\n",
        "        openai.api_key = self.openai_api_key\n",
        "        self.client = openai\n",
        "        logger.info(\"OpenAI client initialized successfully.\")\n",
        "\n",
        "    def validate_input_path(self):\n",
        "        \"\"\"Validate the existence of the input file.\"\"\"\n",
        "        if not os.path.exists(self.input_path):\n",
        "            logger.error(f\"Input file not found at: {self.input_path}\")\n",
        "            raise FileNotFoundError(f\"Input file not found at: {self.input_path}\")\n",
        "        logger.info(f\"Input file located at: {self.input_path}\")\n",
        "\n",
        "    def generate_contextual_summary(self, row, max_retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Generate a conversational summary for a given stock row using Perplexity's API.\n",
        "        Retries the request in case of an error and processes blank columns for NewATL and NewATH.\n",
        "        \"\"\"\n",
        "        # Handle missing or invalid stock column\n",
        "        stock_column = \"Stock\" if \"Stock\" in row else \"Stock Name\"\n",
        "        stock_name = row.get(stock_column, \"Unknown\").strip() if stock_column in row else \"Unknown\"\n",
        "\n",
        "        # Handle blanks or NaN for NewATL and NewATH\n",
        "        new_atl = row.get('NewATL', False)\n",
        "        new_ath = row.get('NewATH', False)\n",
        "\n",
        "        if pd.isna(new_atl):  # If blank or NaN, set to False\n",
        "            new_atl = False\n",
        "        if pd.isna(new_ath):  # If blank or NaN, set to False\n",
        "            new_ath = False\n",
        "\n",
        "        atl_commentary = (\n",
        "            f\"The stock hit a new all-time low today at ₹{row.get('Day Low', 'N/A')}, marking a significant decline.\"\n",
        "            if new_atl and row.get('Day Low') is not None else \"\"\n",
        "        )\n",
        "\n",
        "        ath_commentary = (\n",
        "            f\"Previously, the stock reached an all-time high of ₹{row.get('Day High', 'N/A')}, reflecting its volatility.\"\n",
        "            if new_ath and row.get('Day High') else \"\"\n",
        "        )\n",
        "\n",
        "        # Stellar volumes commentary\n",
        "        stellar_volumes = (\n",
        "            f\"The stock displayed exceptional trading activity today with a daily turnover of ₹{row.get('DailyTurnInCr', 'N/A')} Cr, \"\n",
        "            f\"trading {row.get('PercentFloatTradedPrimaryExchange', 0):.1f}% of its float on the primary exchange, \"\n",
        "            f\"and a volume multiple of {row.get('Day volume multiple of week', 'N/A')} compared to the weekly average.\"\n",
        "            if row.get('DailyTurnInCr', 0) > 100\n",
        "            and row.get('PercentFloatTradedPrimaryExchange', 0) > 10\n",
        "            and row.get('Day volume multiple of week', 0) > 2 else \"\"\n",
        "        )\n",
        "\n",
        "        # Performance summaries\n",
        "        unstellar_quarterly_performance = (\n",
        "            f\"The stock has shown poor quarterly performance with a decline of {row.get('Qtr Change %', 0):.1f}%, \"\n",
        "            f\"indicating challenges over the last quarter.\"\n",
        "            if row.get('Qtr Change %', 0) < -15 else \"\"\n",
        "        )\n",
        "\n",
        "        stellar_quarterly_performance = (\n",
        "            f\"The stock has exhibited stellar quarterly performance with a growth of {row.get('Qtr Change %', 0):.1f}%, \"\n",
        "            f\"indicating strong momentum over the last quarter.\"\n",
        "            if row.get('Qtr Change %', 0) > 15 else \"\"\n",
        "        )\n",
        "\n",
        "        stellar_relative_performance = (\n",
        "            f\"The stock delivered stellar relative returns compared to Nifty500 this month ({row.get('Relative returns vs Nifty500 month%', 0):.1f}%) \"\n",
        "            f\"and this quarter ({row.get('Relative returns vs Nifty500 quarter%', 0):.1f}%).\"\n",
        "            if row.get('Relative returns vs Nifty500 month%', 0) > 15 and row.get('Relative returns vs Nifty500 quarter%', 0) > 15 else \"\"\n",
        "        )\n",
        "\n",
        "        unstellar_relative_performance = (\n",
        "            f\"The stock underperformed significantly compared to Nifty500 this month ({row.get('Relative returns vs Nifty500 month%', 0):.1f}%) \"\n",
        "            f\"and this quarter ({row.get('Relative returns vs Nifty500 quarter%', 0):.1f}%).\"\n",
        "            if row.get('Relative returns vs Nifty500 month%', 0) < -15 and row.get('Relative returns vs Nifty500 quarter%', 0) < -15 else \"\"\n",
        "        )\n",
        "\n",
        "        insider_trading_commentary = (\n",
        "            f\"Additionally, significant insider trading and SAST sales last quarter involved more than {row.get('Insider & SAST Buys Last Quarter', 0)} shares, \"\n",
        "            f\"indicating a potential lack of confidence in the stock's future.\"\n",
        "            if row.get('Insider & SAST Buys Last Quarter', 0) > 100000 else \"\"\n",
        "        )\n",
        "\n",
        "        notable_performance = \" \".join(filter(None, [\n",
        "            stellar_volumes,\n",
        "            unstellar_quarterly_performance,\n",
        "            stellar_quarterly_performance,\n",
        "            stellar_relative_performance,\n",
        "            unstellar_relative_performance,\n",
        "            insider_trading_commentary,\n",
        "            atl_commentary,\n",
        "            ath_commentary\n",
        "        ]))\n",
        "\n",
        "        # Construct the query\n",
        "        query = (\n",
        "            f\"Generate a summary for the following stock data:\\n\\n\"\n",
        "            f\"- Stock: {stock_name}\\n\"\n",
        "            f\"- Day change: {row.get('Day change %', 0):.1f}%\\n\"\n",
        "            f\"- Closing price: ₹{row.get('Current Price', 'N/A')}\\n\"\n",
        "            f\"- Daily turnover: ₹{row.get('DailyTurnInCr', 'N/A')} Cr\\n\"\n",
        "            f\"- Trading volume multiple of the week: {row.get('Day volume multiple of week', 'N/A')}\\n\"\n",
        "            f\"- Percent of float traded on the primary exchange: {row.get('PercentFloatTradedPrimaryExchange', 0):.1f}%\\n\"\n",
        "            f\"- Revenue QoQ Growth: {row.get('Revenue QoQ Growth %', 0):.1f}%\\n\"\n",
        "            f\"- Basic EPS QoQ Growth: {row.get('Basic EPS QoQ Growth %', 0):.1f}%\\n\"\n",
        "            f\"- Day low: ₹{row.get('Day Low', 'N/A')}\\n\"\n",
        "            f\"- Day high: ₹{row.get('Day High', 'N/A')}\\n\"\n",
        "            f\"- New All-Time Low: {'Yes' if new_atl else 'No'}\\n\"\n",
        "            f\"- New All-Time High: {'Yes' if new_ath else 'No'}\\n\"\n",
        "            f\"{notable_performance}\\n\\n\"\n",
        "            f\"Generate a friendly and engaging summary focusing on today's performance, recent decline, notable highs and lows, and overall trends.\\n\\n\"\n",
        "        )\n",
        "\n",
        "        # Retry mechanism (unchanged)\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                payload = {\n",
        "                    \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "                    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
        "                }\n",
        "                response = requests.post(\n",
        "                    \"https://api.perplexity.ai/chat/completions\",\n",
        "                    json=payload,\n",
        "                    headers={\"Authorization\": f\"Bearer {self.perplexity_api_key}\", \"Content-Type\": \"application/json\"}\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                summary = response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No summary generated.\")\n",
        "                logger.info(f\"Perplexity summary generated for {stock_name}.\")\n",
        "                return summary\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Attempt {attempt + 1}/{max_retries} - Error generating summary for {stock_name}: {e}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    return f\"Error generating summary after {max_retries} attempts: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    async def fetch_perplexity(self, session, query, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Fetches data from Perplexity API with retry logic.\n",
        "\n",
        "        Args:\n",
        "            session: The aiohttp session for the request.\n",
        "            query: The query to send to the Perplexity API.\n",
        "            retries: Number of retries in case of failure.\n",
        "            retry_delay: Delay in seconds between retries.\n",
        "\n",
        "        Returns:\n",
        "            dict: The response from the Perplexity API or an error message.\n",
        "        \"\"\"\n",
        "        api_url = \"https://api.perplexity.ai/chat/completions\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        payload = {\n",
        "            \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
        "        }\n",
        "\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                async with session.post(api_url, headers=headers, json=payload) as response:\n",
        "                    if response.status == 200:\n",
        "                        response_data = await response.json()\n",
        "                        logger.info(f\"Perplexity API response received for query: {query}\")\n",
        "                        return response_data\n",
        "                    else:\n",
        "                        logger.error(f\"Perplexity API error: HTTP {response.status}\")\n",
        "                        if attempt < retries - 1:\n",
        "                            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                            await asyncio.sleep(retry_delay)\n",
        "                        else:\n",
        "                            return {\"error\": f\"HTTP {response.status}\"}\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error querying Perplexity API: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    await asyncio.sleep(retry_delay)\n",
        "                else:\n",
        "                    return {\"error\": str(e)}\n",
        "\n",
        "    def fetch_perplexity_results(self, queries, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Fetches Perplexity news results for the given queries with retry logic.\n",
        "\n",
        "        Args:\n",
        "            queries: List of queries to fetch results for.\n",
        "            retries: Number of retries in case of failure.\n",
        "            retry_delay: Delay in seconds between retries.\n",
        "\n",
        "        Returns:\n",
        "            list: List of responses for each query.\n",
        "        \"\"\"\n",
        "        async def fetch_all():\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                tasks = [\n",
        "                    self.fetch_perplexity(session, query, retries=retries, retry_delay=retry_delay)\n",
        "                    for query in queries\n",
        "                ]\n",
        "                return await asyncio.gather(*tasks)\n",
        "\n",
        "        return asyncio.run(fetch_all())\n",
        "\n",
        "    async def fetch_all_news(self, queries, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Fetches all news results using Perplexity API with retry logic.\n",
        "\n",
        "        Args:\n",
        "            queries: List of queries to fetch results for.\n",
        "            retries: Number of retries in case of failure.\n",
        "            retry_delay: Delay in seconds between retries.\n",
        "\n",
        "        Returns:\n",
        "            list: List of responses for each query.\n",
        "        \"\"\"\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [\n",
        "                self.fetch_perplexity(session, query, retries=retries, retry_delay=retry_delay)\n",
        "                for query in queries\n",
        "            ]\n",
        "            return await asyncio.gather(*tasks)\n",
        "\n",
        "    def combine_summaries(self, row):\n",
        "        \"\"\"\n",
        "        Generate a single, coherent, and engaging news byte from contextual and news summaries.\n",
        "        Focuses on high gain and high volume stocks across the market, with a priority on price-impacting factors.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from datetime import datetime\n",
        "            from pytz import timezone\n",
        "            import requests\n",
        "            import re\n",
        "\n",
        "            # Determine market status based on IST\n",
        "            ist = timezone(\"Asia/Kolkata\")\n",
        "            current_time = datetime.now(ist)\n",
        "            market_close_time = current_time.replace(hour=15, minute=30, second=0, microsecond=0)\n",
        "\n",
        "            market_status_message = (\n",
        "                \"This commentary is being generated while the market is open. \"\n",
        "                \"Here are the highlights of high gain and high volume counters in the market today.\"\n",
        "                if current_time < market_close_time else\n",
        "                \"This commentary is being generated after the market has closed. \"\n",
        "                \"Here's a recap of today's high gain and high volume counters in the market.\"\n",
        "            )\n",
        "\n",
        "            # Extract stock information and summaries\n",
        "            stock = row.get(\"Stock\") or row.get(\"Stock Name\", \"Unknown Stock\").strip()\n",
        "            day_change = f\"{row.get('Day change %', 'N/A')}%\"\n",
        "            closing_price = f\"{row.get('Current Price', 'N/A')} rupees\"\n",
        "            turnover = f\"{row.get('DailyTurnInCr', 'N/A')} crore rupees\"\n",
        "            volume_multiple = f\"{row.get('Day volume multiple of week', 'N/A')} times the weekly average\"\n",
        "            float_traded = f\"{row.get('PercentFloatTradedPrimaryExchange', 'N/A')}% of the float traded\"\n",
        "\n",
        "            contextual_summary = row.get('ContextualSummary', 'No contextual summary available.').strip()\n",
        "            perplexity_news = row.get('PerplexityNews', 'No news available.').strip()\n",
        "\n",
        "            # Truncate long summaries\n",
        "            contextual_summary = (contextual_summary[:1000] + \"...\") if len(contextual_summary) > 1000 else contextual_summary\n",
        "            perplexity_news = (perplexity_news[:1000] + \"...\") if len(perplexity_news) > 1000 else perplexity_news\n",
        "\n",
        "            # Adjust commentary to remove unnecessary statements and correct factual inconsistencies\n",
        "            commentary_lines = [\n",
        "                f\"{stock} registered a daily change of {day_change}, closing at {closing_price}.\",\n",
        "                f\"The stock saw a significant turnover of {turnover}, with trading volume reaching {volume_multiple}.\",\n",
        "                f\"Additionally, {float_traded} indicates notable investor activity.\"\n",
        "            ]\n",
        "\n",
        "            if row.get('NewATL', False):\n",
        "                commentary_lines.append(f\"The stock touched a new all-time low of {row.get('Day Low', 'N/A')} rupees today.\")\n",
        "            if row.get('NewATH', False):\n",
        "                commentary_lines.append(f\"It also hit a new all-time high of {row.get('Day High', 'N/A')} rupees today.\")\n",
        "\n",
        "            # Construct the input for the news byte\n",
        "            combined_input = (\n",
        "                f\"{market_status_message}\\n\\n\"\n",
        "                f\"\\n\".join(commentary_lines) + \"\\n\\n\"\n",
        "                f\"### Contextual Highlights:\\n\"\n",
        "                f\"{contextual_summary}\\n\\n\"\n",
        "                f\"### News Highlights:\\n\"\n",
        "                f\"{perplexity_news}\\n\\n\"\n",
        "                f\"### Instructions for News Byte Generation:\\n\"\n",
        "                f\"Craft an engaging and concise news byte based on the provided details. The output should:\\n\"\n",
        "                f\"- **Highlight Volumes and Turnover:** Focus on significant trading volumes, turnover, and float traded percentages.\\n\"\n",
        "                f\"- **Analyze Price Impact:** Connect price changes to key metrics like revenue growth, EPS, partnerships, or other impactful events.\\n\"\n",
        "                f\"- **Emphasize Investor Sentiment:** Discuss how market sentiment influenced the stock's performance today.\\n\"\n",
        "                f\"- **Provide Context:** Relate the movement to specific events or announcements.\\n\\n\"\n",
        "                f\"### Output Requirements:\\n\"\n",
        "                f\"- Use a conversational tone and ensure clarity for a general audience.\\n\"\n",
        "                f\"- Avoid markdowns, citations, or links.\\n\"\n",
        "                f\"- Expand abbreviations and replace '₹' with 'rupees.'\\n\"\n",
        "                f\"- Limit to under 500 words for suitability in a 2-3 minute video format.\\n\"\n",
        "                f\"End with a note inviting users to stay tuned for more market updates.\"\n",
        "            )\n",
        "\n",
        "            # API call to generate the summary\n",
        "            payload = {\n",
        "                \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": combined_input}]\n",
        "            }\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "\n",
        "            response = requests.post(\"https://api.perplexity.ai/chat/completions\", json=payload, headers=headers)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                summary = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No summary generated.\")\n",
        "                return re.sub(r\"\\[\\d+\\]\", \"\", summary).strip()  # Clean markdown-like citations\n",
        "            else:\n",
        "                logger.error(f\"API Error: {response.status_code} - {response.text}\")\n",
        "                return f\"Error generating summary: {response.status_code}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in combine_summaries: {e}\")\n",
        "            return f\"Error combining summaries: {e}\"\n",
        "\n",
        "\n",
        "    def generate_initial_news_byte(self, df, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Generates the initial news byte from combined summaries and daily changes with retry logic.\n",
        "        Focuses on high gain and high volume stocks across the market, emphasizing Day Change % and turnover.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Determine the stock column dynamically\n",
        "            stock_column = \"Stock\" if \"Stock\" in df.columns else \"Stock Name\"\n",
        "            if stock_column not in df.columns:\n",
        "                logger.error(\"Neither 'Stock' nor 'Stock Name' column found in the DataFrame.\")\n",
        "                return \"Error: Required column 'Stock' or 'Stock Name' not found.\"\n",
        "\n",
        "            # Prepare the stock changes with high gain and high volume context\n",
        "            stock_changes = \"\\n\".join(\n",
        "                f\"{row.get(stock_column, 'Unknown Stock')} saw a gain of {row.get('Day change %', 'N/A')}% \"\n",
        "                f\"with a turnover of ₹{row.get('DailyTurnInCr', 'N/A')} crore and {row.get('PercentFloatTradedPrimaryExchange', 'N/A')}% of its float traded.\"\n",
        "                for _, row in df.iterrows()\n",
        "            )\n",
        "            logger.debug(f\"Stock changes prepared: {stock_changes}\")\n",
        "\n",
        "            # Determine market status\n",
        "            ist = timezone(\"Asia/Kolkata\")\n",
        "            current_time = datetime.now(ist)\n",
        "            market_close_time = current_time.replace(hour=15, minute=30, second=0, microsecond=0)\n",
        "\n",
        "            market_status_message = (\n",
        "                \"This commentary is being generated while the market is open. \"\n",
        "                \"Here are today's high gain and high volume stocks across the market.\"\n",
        "                if current_time < market_close_time else\n",
        "                \"This commentary is being generated after the market has closed. \"\n",
        "                \"Here's a summary of today's high gain and high volume stocks across the market.\"\n",
        "            )\n",
        "            logger.debug(f\"Market status message prepared: {market_status_message}\")\n",
        "\n",
        "            combined_input = (\n",
        "                f\"Generate a compelling and engaging news byte summarizing today's high gain and high volume stocks across the market. \"\n",
        "                f\"Focus on significant trading metrics, emphasizing Day Change %, turnover, and float traded.\\n\\n\"\n",
        "                f\"Stocks: {', '.join(df[stock_column])}\\n\\n\"\n",
        "                f\"Performance Summary:\\n{stock_changes}\\n\\n\"\n",
        "                f\"Market Status:\\n{market_status_message}\\n\\n\"\n",
        "                f\"Structure the response as a seamless narrative, adhering to these guidelines:\\n\\n\"\n",
        "                f\"1. **Stock-Specific Highlights:**\\n\"\n",
        "                f\"   - Explicitly mention each stock name followed by its Day Change %, turnover, and float traded. Example: 'Reliance Industries saw a gain of 2.5% with a turnover of ₹500 crore and 12% of its float traded.'\\n\\n\"\n",
        "                f\"2. **Analyze Key Metrics:**\\n\"\n",
        "                f\"   - Explain notable metrics like revenue growth, EPS, or partnerships that impacted performance.\\n\"\n",
        "                f\"   - Example: 'Reliance Industries' performance was driven by a 20% surge in quarterly revenue, following its expansion into renewable energy.'\\n\\n\"\n",
        "                f\"3. **Provide Market Context:**\\n\"\n",
        "                f\"   - Highlight any industry trends, macroeconomic factors, or global developments influencing the stock's movement.\\n\"\n",
        "                f\"   - Example: 'The broader market sentiment improved on hopes of an interest rate pause, boosting large-cap stocks.'\\n\\n\"\n",
        "                f\"4. **Formatting and Tone:**\\n\"\n",
        "                f\"   - Avoid bullet points in the output. Write in a cohesive narrative style.\\n\"\n",
        "                f\"   - Use full forms like 'Limited' instead of 'Ltd.' and replace '₹' with 'rupees.'\\n\\n\"\n",
        "                f\"5. **Conciseness and Engagement:**\\n\"\n",
        "                f\"   - Keep the response under 500 words, ensuring it fits a 2-3 minute video format.\\n\"\n",
        "                f\"   - Maintain an accessible, conversational tone throughout.\\n\\n\"\n",
        "                f\"Deliver the response as a polished and engaging news byte with each stock's performance and context comprehensively covered.\"\n",
        "            )\n",
        "\n",
        "            # Payload for the Perplexity API\n",
        "            payload = {\n",
        "                \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": combined_input}],\n",
        "                \"max_tokens\": 1500\n",
        "            }\n",
        "\n",
        "            # Retry logic for API requests\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    response = requests.post(self.api_url, json=payload, headers=self.headers)\n",
        "                    if response.status_code == 200:\n",
        "                        result = response.json()\n",
        "                        news_byte = result.get('choices', [{}])[0].get('message', {}).get('content', \"No news byte generated.\")\n",
        "\n",
        "                        # Clean the generated news byte\n",
        "                        cleaned_news_byte = re.sub(r\"\\[\\d+\\]\", \"\", news_byte)  # Remove citations like [1], [2]\n",
        "                        cleaned_news_byte = re.sub(r\"\\*\\*.*?\\*\\*\", \"\", cleaned_news_byte).strip()  # Remove markdown formatting\n",
        "                        logger.info(\"Initial news byte generated successfully.\")\n",
        "                        return cleaned_news_byte\n",
        "                    else:\n",
        "                        logger.error(f\"Perplexity API error: {response.status_code} - {response.text}\")\n",
        "                        if attempt < retries - 1:\n",
        "                            logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                            time.sleep(retry_delay)\n",
        "                        else:\n",
        "                            return f\"Error generating initial news byte: {response.status_code}\"\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Exception generating initial news byte on attempt {attempt + 1}: {e}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                        time.sleep(retry_delay)\n",
        "                    else:\n",
        "                        return f\"Error generating initial news byte: {e}\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error in generate_initial_news_byte: {e}\")\n",
        "            return f\"Error generating initial news byte: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "    def shorten_news_byte(self, initial_news_byte, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Shortens the news byte if it exceeds 500 words, with retry logic.\n",
        "        Ensures that stock names and high gain/high volume updates are preserved in the shortened version.\n",
        "        \"\"\"\n",
        "        shorten_request = (\n",
        "            \"The previous response exceeded 500 words. Please provide a shorter version \"\n",
        "            \"of the following news byte, strictly under 500 words while retaining all key information. \"\n",
        "            \"Focus specifically on high gain and high volume stocks. It is essential to preserve the stock names and \"\n",
        "            \"explicitly mention their performance metrics such as Day Change %, turnover, and float traded. \"\n",
        "            \"For example, include phrases like 'Reliance Industries gained 2.5% with a turnover of rupees 500 crore' \"\n",
        "            \"or 'Tata Steel surged 3.2% trading 15% of its float.' Each stock name must be clearly mentioned.\\n\\n\"\n",
        "            f\"{initial_news_byte}\\n\\n\"\n",
        "            \"Key Instructions:\\n\"\n",
        "            \"- Retain all stock names and explicitly mention their high gain/high volume performance.\\n\"\n",
        "            \"- Include critical metrics such as turnover, float traded, or relevant news that explains the movement of each stock.\\n\"\n",
        "            \"- Avoid generalizations or omitting stock-specific details.\\n\"\n",
        "            \"- Use engaging and conversational language suitable for a 2-3 minute video format.\\n\"\n",
        "            \"- Remove redundant phrases and ensure the response is concise.\\n\"\n",
        "            \"- Avoid citations (e.g., [1], [2]) or markdown-style formatting (e.g., '**text**').\\n\"\n",
        "            \"- Replace symbols like '₹' with 'rupees' and expand 'Ltd.' to 'Limited.'\\n\"\n",
        "            \"- Ensure numerical ranges (e.g., 80-100) are written as '80 to 100.'\\n\"\n",
        "            \"- Ensure all content is under 500 words and easy to understand.\"\n",
        "        )\n",
        "\n",
        "        payload = {\n",
        "            \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": shorten_request}],\n",
        "            \"max_tokens\": 1500\n",
        "        }\n",
        "\n",
        "        # Retry logic\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = requests.post(self.api_url, json=payload, headers=self.headers)\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    news_byte = result.get('choices', [{}])[0].get('message', {}).get('content', \"No news byte generated.\")\n",
        "\n",
        "                    # Clean the shortened news byte\n",
        "                    cleaned_news_byte = re.sub(r\"\\*\\*.*?\\*\\*\", \"\", news_byte).strip()\n",
        "                    logger.info(\"Shortened news byte generated successfully.\")\n",
        "                    return cleaned_news_byte\n",
        "                else:\n",
        "                    logger.error(f\"Perplexity API error: {response.status_code} - {response.text}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                        time.sleep(retry_delay)\n",
        "                    else:\n",
        "                        return f\"Error shortening news byte: {response.status_code}\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Exception shortening news byte: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    return f\"Error shortening news byte: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "    def generate_final_news_byte(self, df):\n",
        "        \"\"\"\n",
        "        Generates the final news byte, shortening it if necessary.\n",
        "        Focuses on high gain and high volume stocks while retaining stock names and key updates.\n",
        "        Removes citations like [1], [2], and ensures a clean, concise output.\n",
        "        Includes retries in case of errors and comprehensive logging.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Ensure the stock column exists dynamically\n",
        "            stock_column = \"Stock\" if \"Stock\" in df.columns else \"Stock Name\"\n",
        "            if stock_column not in df.columns:\n",
        "                logger.error(\"Neither 'Stock' nor 'Stock Name' column found in the DataFrame.\")\n",
        "                return \"Error: Required column 'Stock' or 'Stock Name' not found.\"\n",
        "\n",
        "            # Generate the initial news byte\n",
        "            logger.info(\"Starting to generate the initial news byte.\")\n",
        "            initial_news_byte = self.generate_initial_news_byte(df)\n",
        "\n",
        "            if not initial_news_byte or \"Error\" in initial_news_byte:\n",
        "                logger.error(\"Failed to generate the initial news byte.\")\n",
        "                return \"Error generating the initial news byte.\"\n",
        "\n",
        "            # Clean citations from the initial news byte\n",
        "            cleaned_initial_news_byte = re.sub(r\"\\[\\d+\\]\", \"\", initial_news_byte)  # Remove citations like [1], [2]\n",
        "            logger.info(\"Cleaned citations from the initial news byte.\")\n",
        "\n",
        "            # Calculate word count of the cleaned initial news byte\n",
        "            word_count = len(cleaned_initial_news_byte.split())\n",
        "            logger.info(f\"Initial news byte generated with {word_count} words after cleaning.\")\n",
        "\n",
        "            # Define the word limit\n",
        "            WORD_LIMIT = 500\n",
        "\n",
        "            # If within the word limit, return the cleaned initial news byte\n",
        "            if word_count <= WORD_LIMIT:\n",
        "                logger.info(\"Initial news byte is within the word limit. Returning the final news byte.\")\n",
        "                return cleaned_initial_news_byte\n",
        "\n",
        "            # Log and attempt to shorten the news byte\n",
        "            logger.warning(f\"Initial news byte exceeded {WORD_LIMIT} words ({word_count}). Attempting to shorten.\")\n",
        "            max_retries = 3\n",
        "            retry_delay = 2  # Delay between retries in seconds\n",
        "            shortened_news_byte = None\n",
        "\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    logger.info(f\"Attempt {attempt + 1}/{max_retries} to shorten the news byte.\")\n",
        "                    shortened_news_byte = self.shorten_news_byte(cleaned_initial_news_byte)\n",
        "\n",
        "                    # Clean citations from the shortened news byte\n",
        "                    cleaned_shortened_news_byte = re.sub(r\"\\[\\d+\\]\", \"\", shortened_news_byte)  # Remove citations\n",
        "                    final_word_count = len(cleaned_shortened_news_byte.split())\n",
        "                    logger.info(f\"Shortened news byte generated with {final_word_count} words after cleaning.\")\n",
        "\n",
        "                    # Check if the shortened news byte is within the word limit\n",
        "                    if final_word_count <= WORD_LIMIT:\n",
        "                        logger.info(\"Shortened news byte is within the word limit. Returning the final news byte.\")\n",
        "                        return cleaned_shortened_news_byte\n",
        "                    else:\n",
        "                        logger.warning(f\"Shortened news byte still exceeds the word limit ({final_word_count} words).\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error shortening the news byte on attempt {attempt + 1}: {e}\")\n",
        "                    if attempt < max_retries - 1:\n",
        "                        logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                        time.sleep(retry_delay)\n",
        "\n",
        "            # If retries are exhausted, return the cleaned initial news byte with a warning\n",
        "            logger.warning(\"Failed to shorten the news byte within the word limit after multiple attempts.\")\n",
        "            return f\"The news byte exceeds the word limit but here is the unshortened version:\\n\\n{cleaned_initial_news_byte}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error in generating the final news byte: {e}\")\n",
        "            return \"Error generating the final news byte.\"\n",
        "\n",
        "\n",
        "    def refine_final_news_byte(self, df, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Refines the `FinalNewsByte` column for stocks already present using ChatGPT API.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): The DataFrame containing the `FinalNewsByte` column.\n",
        "            retries (int): Number of retries in case of API errors.\n",
        "            retry_delay (int): Delay in seconds between retries.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Updated DataFrame with refined `FinalNewsByte`.\n",
        "        \"\"\"\n",
        "        if 'FinalNewsByte' not in df.columns:\n",
        "            logger.error(\"`FinalNewsByte` column not found in DataFrame.\")\n",
        "            return df\n",
        "\n",
        "        if df['FinalNewsByte'].isna().all():\n",
        "            logger.error(\"`FinalNewsByte` column is empty or contains only NaN values.\")\n",
        "            return df\n",
        "\n",
        "        # Ensure only stocks present in the DataFrame are referenced\n",
        "        stock_names = df['Stock'].dropna().unique().tolist()\n",
        "\n",
        "        # Prepare the news byte for refinement\n",
        "        final_news_byte = df['FinalNewsByte'].iloc[0]  # Assumes all rows share the same content\n",
        "\n",
        "        if not isinstance(final_news_byte, str):\n",
        "            logger.error(\"Invalid type for FinalNewsByte. Expected string.\")\n",
        "            df['FinalNewsByte'] = \"Error: Invalid FinalNewsByte format.\"\n",
        "            return df\n",
        "\n",
        "        refine_request = (\n",
        "            \"Refine the following news byte about today's stock market updates to make it more engaging and professional, \"\n",
        "            \"suitable for a financial audience. Emphasize key metrics, highlight significant trends, and ensure a conversational tone. \"\n",
        "            \"Focus only on the stocks mentioned explicitly and avoid introducing additional stocks or irrelevant details:\\n\\n\"\n",
        "            f\"Stocks: {', '.join(stock_names)}\\n\"\n",
        "            f\"{final_news_byte}\\n\\n\"\n",
        "            \"Do not include markdown formatting or any instructions. Focus on providing a polished and publication-ready summary.\"\n",
        "        )\n",
        "\n",
        "        payload = {\n",
        "            \"model\": \"gpt-4\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": refine_request}],\n",
        "            \"max_tokens\": 1024\n",
        "        }\n",
        "\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = requests.post(\"https://api.openai.com/v1/chat/completions\",\n",
        "                                        json=payload,\n",
        "                                        headers={\"Authorization\": f\"Bearer {self.openai_api_key}\", \"Content-Type\": \"application/json\"})\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    refined_news_byte = result.get('choices', [{}])[0].get('message', {}).get('content', \"No refinement generated.\")\n",
        "                    refined_news_byte = refined_news_byte.strip()\n",
        "                    logger.info(\"News byte refined successfully.\")\n",
        "                    df['FinalNewsByte'] = refined_news_byte  # Update all rows with the refined content\n",
        "                    return df\n",
        "                else:\n",
        "                    logger.error(f\"ChatGPT API error: {response.status_code} - {response.text}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                        time.sleep(retry_delay)\n",
        "                    else:\n",
        "                        df['FinalNewsByte'] = \"Error refining news byte.\"\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Exception during refinement: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    df['FinalNewsByte'] = \"Error refining news byte.\"\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def shorten_summary_for_reel(self, summary, max_tokens=500, retries=3, retry_delay=2):\n",
        "        \"\"\"\n",
        "        Shortens the provided summary to fit within the token limit for a 1-minute reel.\n",
        "\n",
        "        Args:\n",
        "            summary (str): The input summary to be shortened.\n",
        "            max_tokens (int): Maximum tokens allowed for the shortened summary.\n",
        "            retries (int): Number of retry attempts in case of API errors.\n",
        "            retry_delay (int): Delay (in seconds) between retries.\n",
        "\n",
        "        Returns:\n",
        "            str: The shortened summary.\n",
        "        \"\"\"\n",
        "        logger.info(\"Shortening summary for 1-minute reel focused on high gain and high volume counters.\")\n",
        "\n",
        "        shorten_request = (\n",
        "            \"Please shorten the following summary to fit within a 1-minute video duration. \"\n",
        "            f\"Limit the response to {max_tokens} tokens, retaining all key points, especially focusing on high gain and high volume stocks, and making it conversational:\\n\\n\"\n",
        "            f\"{summary}\\n\\n\"\n",
        "            \"Key Instructions:\\n\"\n",
        "            \"- Mention all stock names explicitly alongside their % Day Change, turnover, and float traded.\\n\"\n",
        "            \"- Retain critical updates like revenue growth, partnerships, or market sentiment linked to the stocks.\\n\"\n",
        "            \"- Avoid markdown formatting and citations like [1], [2].\\n\"\n",
        "            \"- Replace symbols like '₹' with 'rupees' and expand abbreviations such as 'Ltd.' to 'Limited.'\\n\"\n",
        "            \"- Ensure the language is concise, engaging, and suitable for a 1-minute reel format.\"\n",
        "        )\n",
        "\n",
        "        payload = {\n",
        "            \"model\": \"llama-3.1-sonar-huge-128k-online\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": shorten_request}],\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "        # Retry logic\n",
        "        for attempt in range(retries):\n",
        "            try:\n",
        "                response = requests.post(self.api_url, json=payload, headers=self.headers)\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    shortened_summary = result.get('choices', [{}])[0].get('message', {}).get('content', \"No summary generated.\")\n",
        "\n",
        "                    # Clean the shortened summary\n",
        "                    cleaned_summary = re.sub(r\"\\*\\*.*?\\*\\*\", \"\", shortened_summary).strip()\n",
        "                    logger.info(\"Reel summary successfully shortened.\")\n",
        "                    return cleaned_summary\n",
        "                else:\n",
        "                    logger.error(f\"Perplexity API error: {response.status_code} - {response.text}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                        time.sleep(retry_delay)\n",
        "                    else:\n",
        "                        return f\"Error shortening summary: {response.status_code}\"\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Exception shortening summary: {e}\")\n",
        "                if attempt < retries - 1:\n",
        "                    logger.info(f\"Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    return f\"Error shortening summary: {e}\"\n",
        "\n",
        "\n",
        "    def generate_reel_byte(self, df):\n",
        "        \"\"\"\n",
        "        Generates a 1-minute ReelByte for individual stocks and combined news, focusing on high gain and high volume updates.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing stock data with summaries.\n",
        "\n",
        "        Adds:\n",
        "            ReelByteIndividual (list): 1-minute summaries for individual stocks.\n",
        "            ReelByteCombined (str): 1-minute summary for the combined news.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(\"Generating 1-minute ReelBytes for high gain and high volume stocks.\")\n",
        "\n",
        "            # Check input type\n",
        "            if not isinstance(df, pd.DataFrame):\n",
        "                logger.error(f\"Invalid input to generate_reel_byte: Expected DataFrame, got {type(df)}\")\n",
        "                return\n",
        "\n",
        "            # Generate ReelByte for individual stocks\n",
        "            reel_summaries = []\n",
        "            for summary in df['CombinedSummary']:\n",
        "                if not isinstance(summary, str):\n",
        "                    logger.error(f\"Invalid type for summary: {type(summary)}. Expected a string.\")\n",
        "                    reel_summaries.append(\"Error: Invalid summary format.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    reel_summary = self.shorten_summary_for_reel(summary)\n",
        "                    reel_summaries.append(reel_summary)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error shortening summary for reel: {e}\")\n",
        "                    reel_summaries.append(\"Error: Could not generate ReelByte.\")\n",
        "\n",
        "            df['ReelByteIndividual'] = reel_summaries\n",
        "\n",
        "            # Generate ReelByte for the combined news summary\n",
        "            if 'FinalNewsByte' in df.columns:\n",
        "                combined_news_summary = df['FinalNewsByte'].iloc[0]  # Assume all rows have identical FinalNewsByte\n",
        "                if not isinstance(combined_news_summary, str):\n",
        "                    logger.error(f\"Invalid type for FinalNewsByte: {type(combined_news_summary)}. Expected a string.\")\n",
        "                    reel_combined_news = \"Error: Invalid FinalNewsByte format.\"\n",
        "                else:\n",
        "                    try:\n",
        "                        reel_combined_news = self.shorten_summary_for_reel(combined_news_summary)\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error shortening combined news for reel: {e}\")\n",
        "                        reel_combined_news = \"Error: Could not generate ReelByte.\"\n",
        "            else:\n",
        "                logger.error(\"FinalNewsByte column not found in DataFrame.\")\n",
        "                reel_combined_news = \"Error: FinalNewsByte not found.\"\n",
        "\n",
        "            df['ReelByteCombined'] = [reel_combined_news] * len(df)\n",
        "\n",
        "            logger.info(\"1-minute ReelBytes successfully generated.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating ReelByte: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def upload_image_to_s3(self, file_path, title):\n",
        "        \"\"\"\n",
        "        Uploads an image file to an S3 bucket and returns the public URL of the uploaded file.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            aws_access_key_id = userdata.get(\"aws_access_key_id\")\n",
        "            aws_secret_access_key = userdata.get(\"aws_secret_access_key\")\n",
        "\n",
        "            if not aws_access_key_id or not aws_secret_access_key:\n",
        "                raise Exception(\"AWS credentials are not set in Colab userdata.\")\n",
        "\n",
        "            # Create an S3 client\n",
        "            s3 = boto3.client(\n",
        "                \"s3\",\n",
        "                aws_access_key_id=aws_access_key_id,\n",
        "                aws_secret_access_key=aws_secret_access_key,\n",
        "                region_name=\"ap-south-1\",\n",
        "            )\n",
        "\n",
        "            # Get the current date and time in a file-safe format\n",
        "            current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "            # Generate the S3 file key with datetime\n",
        "            sanitized_title = \"\".join(e for e in title if e.isalnum() or e == \"_\").replace(\" \", \"_\").lower()\n",
        "            file_key = f\"images/{sanitized_title}_{current_datetime}.png\"\n",
        "\n",
        "            # Upload the image file to the S3 bucket\n",
        "            s3.upload_file(file_path, \"finbytes\", file_key)\n",
        "\n",
        "            # Construct the public URL for the uploaded file\n",
        "            file_url = f\"https://finbytes.s3.ap-south-1.amazonaws.com/{file_key}\"\n",
        "            logger.info(f\"Image uploaded to S3 successfully. URL: {file_url}\")\n",
        "            return file_url\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to upload image to S3: {str(e)}\")\n",
        "            raise Exception(f\"Failed to upload image to S3: {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "    def generate_plot(self, df):\n",
        "        try:\n",
        "            # Dynamically determine the stock column\n",
        "            stock_column = \"Stock\" if \"Stock\" in df.columns else \"Stock Name\"\n",
        "            if stock_column not in df.columns:\n",
        "                logger.error(\"Neither 'Stock' nor 'Stock Name' column found in the DataFrame.\")\n",
        "                return\n",
        "\n",
        "            # Wrap long stock names for better display\n",
        "            df[stock_column] = df[stock_column].apply(lambda x: fill(x, width=15))\n",
        "\n",
        "            # Add a helper column for color coding\n",
        "            df['Color'] = df['Day change %'].apply(lambda x: '#008000' if x > 0 else '#FF0000')  # Green for positive, Red for negative\n",
        "\n",
        "            # Set the aesthetic style of the plots\n",
        "            sns.set_theme(style=\"ticks\")  # Use 'ticks' to enable control of spines\n",
        "\n",
        "            # Create the figure with mobile-friendly dimensions\n",
        "            plt.figure(figsize=(6, 10))  # Adjust aspect ratio (width x height)\n",
        "\n",
        "            # Create the horizontal bar plot\n",
        "            bars = sns.barplot(\n",
        "                x='Day change %',\n",
        "                y=stock_column,\n",
        "                data=df,\n",
        "                palette=df['Color'].tolist(),  # Apply precomputed colors\n",
        "                orient='h'\n",
        "            )\n",
        "\n",
        "            # Add data labels on the bars\n",
        "            for bar in bars.patches:\n",
        "                bars.annotate(\n",
        "                    f\"{bar.get_width():.1f}%\",  # Format to 1 decimal place\n",
        "                    (bar.get_width() + 0.2, bar.get_y() + bar.get_height() / 2),  # Position the text\n",
        "                    ha='left', va='center', fontsize=10, fontweight='bold', color='white', xytext=(5, 0),\n",
        "                    textcoords='offset points'\n",
        "                )\n",
        "\n",
        "            # Remove gridlines and retain only left and bottom axes\n",
        "            sns.despine(left=False, bottom=False)  # Retain left and bottom borders\n",
        "            plt.gca().grid(False)  # Turn off all gridlines\n",
        "\n",
        "            # Set the main title\n",
        "            plt.title('Daily % Change of Stocks', fontsize=16, fontweight='bold', pad=15, color='white')\n",
        "\n",
        "            # Set axis labels\n",
        "            plt.xlabel('Daily % Change', fontsize=12, fontweight='bold', color='white')\n",
        "            plt.ylabel('Stocks', fontsize=12, fontweight='bold', color='white')\n",
        "\n",
        "            # Adjust font size and make stock names bold\n",
        "            plt.xticks(fontsize=10, fontweight='bold', color='white')\n",
        "            plt.yticks(fontsize=10, fontweight='bold', ha='right', color='white')  # Align stock names to the left\n",
        "\n",
        "            # Set background color to black\n",
        "            plt.gca().set_facecolor('black')\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot with black background\n",
        "            plt.savefig(self.plot_path, dpi=200, facecolor='black')  # Ensure the background is saved as black\n",
        "            plt.close()\n",
        "\n",
        "            logger.info(f\"Mobile-friendly horizontal bar plot saved to {self.plot_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating horizontal bar plot: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_summary_as_json(self, final_news_byte, plot_url):\n",
        "        \"\"\"\n",
        "        Save the final combined news byte and metadata as a JSON file.\n",
        "        \"\"\"\n",
        "        # Get current time in IST (UTC+5:30)\n",
        "        ist = timezone(\"Asia/Kolkata\")\n",
        "        created_time = datetime.now(ist).isoformat()  # Generate ISO format with timezone\n",
        "\n",
        "        # JSON output\n",
        "        output = {\n",
        "            \"created_time\": created_time,  # Use IST with UTC+5:30 offset\n",
        "            \"title\": \"High Gain & High Volumes Today\",\n",
        "            \"summary_text\": final_news_byte,\n",
        "            \"image_url\": plot_url,  # Include the uploaded plot URL\n",
        "        }\n",
        "\n",
        "        # Save the JSON to a file\n",
        "        json_path = \"/content/HighGainHighVolume.json\"\n",
        "        try:\n",
        "            with open(json_path, \"w\") as f:\n",
        "                json.dump(output, f, indent=4)\n",
        "            logger.info(f\"JSON summary saved to {json_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving JSON summary to {json_path}: {e}\")\n",
        "\n",
        "\n",
        "    def post_json_to_summary(self, json_data):\n",
        "        \"\"\"\n",
        "        Posts a JSON summary to the specified API endpoint.\n",
        "\n",
        "        Args:\n",
        "            json_data (dict): The JSON data to be posted to the API.\n",
        "\n",
        "        Returns:\n",
        "            dict: The response from the API if successful.\n",
        "\n",
        "        Raises:\n",
        "            Exception: If the POST request fails.\n",
        "        \"\"\"\n",
        "        api_url = \"https://sy6foef31c.execute-api.ap-south-1.amazonaws.com/default/create\"\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "        try:\n",
        "            response = requests.post(api_url, json=json_data, headers=headers)\n",
        "            response.raise_for_status()  # Raise an error for non-2xx responses\n",
        "            logger.info(f\"POST request successful. Response: {response.json()}\")\n",
        "            return response.json()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error posting JSON to API: {e}\")\n",
        "            raise\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main method to execute the Top Gainers and Losers process.\"\"\"\n",
        "        logger.info(\"Starting the Top Gainers and Losers process.\")\n",
        "\n",
        "        # Step 1: Read and preprocess the input CSV\n",
        "        try:\n",
        "            df = pd.read_csv(self.input_path)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Input file not found at {self.input_path}.\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading input file: {e}\")\n",
        "            return\n",
        "\n",
        "        # Validate DataFrame\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            logger.error(\"Invalid input file. Expected a DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Determine the column to use for stock names\n",
        "        stock_column = None\n",
        "        if \"Stock\" in df.columns:\n",
        "            stock_column = \"Stock\"\n",
        "        elif \"Stock Name\" in df.columns:\n",
        "            stock_column = \"Stock Name\"\n",
        "\n",
        "        if not stock_column:\n",
        "            logger.error(\"Neither 'Stock' nor 'Stock Name' columns are present in the input file.\")\n",
        "            return\n",
        "\n",
        "        # Ensure the necessary columns exist\n",
        "        required_columns = [stock_column, \"DailyTurnInCr\"]\n",
        "        missing_columns = set(required_columns) - set(df.columns)\n",
        "        if missing_columns:\n",
        "            logger.error(f\"Missing required columns: {missing_columns}. Ensure the input file includes these columns.\")\n",
        "            return\n",
        "\n",
        "        # Handle missing or invalid values in the DataFrame\n",
        "        df[stock_column] = df[stock_column].fillna('Unknown')\n",
        "        df['DailyTurnInCr'] = pd.to_numeric(df['DailyTurnInCr'], errors='coerce').fillna(0)\n",
        "\n",
        "        # Step 2: Process top 4 stocks based on \"DailyTurnInCr\"\n",
        "        logger.info(\"Selecting top 4 stocks by DailyTurnInCr.\")\n",
        "        try:\n",
        "            df = df.nlargest(4, \"DailyTurnInCr\").sort_values(by=\"DailyTurnInCr\", ascending=False)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error selecting top stocks by DailyTurnInCr: {e}\")\n",
        "            return\n",
        "\n",
        "        # Step 3: Generate Contextual summaries\n",
        "        logger.info(\"Generating Contextual summaries.\")\n",
        "        try:\n",
        "            df['ContextualSummary'] = df.apply(self.generate_contextual_summary, axis=1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating contextual summaries: {e}\")\n",
        "            df['ContextualSummary'] = [\"Error generating summary\" for _ in range(len(df))]\n",
        "\n",
        "        # Step 4: Fetch Perplexity news\n",
        "        logger.info(\"Fetching news from Perplexity.\")\n",
        "        try:\n",
        "            perplexity_queries = [\n",
        "                f\"Find recent news and updates about {row[stock_column]} in the last week.\"\n",
        "                for _, row in df.iterrows() if row[stock_column] != 'Unknown'\n",
        "            ]\n",
        "            perplexity_results = self.fetch_perplexity_results(perplexity_queries)\n",
        "            df['PerplexityNews'] = [\n",
        "                res.get(\"choices\", [{\"message\": {\"content\": \"No recent news found.\"}}])[0][\"message\"][\"content\"]\n",
        "                if isinstance(res, dict) and \"error\" not in res else \"Error occurred\"\n",
        "                for res in perplexity_results\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching news from Perplexity: {e}\")\n",
        "            df['PerplexityNews'] = [\"Error fetching news\" for _ in range(len(df))]\n",
        "\n",
        "        # Step 5: Combine ChatGPT summaries with Perplexity news\n",
        "        logger.info(\"Combining Contextual summaries and Perplexity news.\")\n",
        "        try:\n",
        "            df['CombinedSummary'] = df.apply(self.combine_summaries, axis=1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error combining summaries: {e}\")\n",
        "            df['CombinedSummary'] = [\"Error combining summaries\" for _ in range(len(df))]\n",
        "\n",
        "        # Step 6: Generate a single final news byte covering all stocks\n",
        "        logger.info(\"Generating final news byte for all stocks.\")\n",
        "        try:\n",
        "            final_news_byte = self.generate_final_news_byte(df)\n",
        "            df['FinalNewsByte'] = [final_news_byte for _ in range(len(df))]\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating final news byte: {e}\")\n",
        "            final_news_byte = \"Error generating final news byte.\"\n",
        "            df['FinalNewsByte'] = [final_news_byte for _ in range(len(df))]\n",
        "\n",
        "        # Step 7: Refine the FinalNewsByte\n",
        "        logger.info(\"Refining the FinalNewsByte.\")\n",
        "        try:\n",
        "            df = self.refine_final_news_byte(df)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error refining FinalNewsByte: {e}\")\n",
        "            df['FinalNewsByte'] = [\"Error refining FinalNewsByte\" for _ in range(len(df))]\n",
        "\n",
        "        # Step 8: Generate a concise ReelByte\n",
        "        logger.info(\"Generating a 1-minute ReelByte for individual stocks and combined news.\")\n",
        "        try:\n",
        "            self.generate_reel_byte(df)\n",
        "            logger.info(\"ReelByte generated successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating ReelByte: {e}\")\n",
        "\n",
        "        # Step 9: Generate and save the plot\n",
        "        logger.info(\"Generating and saving plot.\")\n",
        "        try:\n",
        "            self.generate_plot(df)\n",
        "            logger.info(\"Uploading the plot image to S3.\")\n",
        "            plot_url = self.upload_image_to_s3(self.plot_path, \"High Gain High Volume Today\")\n",
        "            logger.info(f\"Plot image successfully uploaded to S3. URL: {plot_url}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating or uploading plot: {e}\")\n",
        "            plot_url = None\n",
        "\n",
        "        # Step 10: Save JSON Summary Locally\n",
        "        logger.info(\"Saving JSON summary locally.\")\n",
        "        try:\n",
        "            self.save_summary_as_json(final_news_byte, plot_url)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving JSON summary: {e}\")\n",
        "\n",
        "        # Step 11: Save the DataFrame to CSV\n",
        "        logger.info(\"Saving the final output CSV.\")\n",
        "        try:\n",
        "            df.to_csv(self.output_path, index=False)\n",
        "            logger.info(f\"Output saved to {self.output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving the output CSV: {e}\")\n",
        "\n",
        "        # # Step 12: Post JSON to API\n",
        "        # try:\n",
        "        #     logger.info(\"Posting JSON data to API.\")\n",
        "        #     json_data = {  # Ensure json_data is properly prepared\n",
        "        #         \"created_time\": get_ist_timestamp(),\n",
        "        #         \"title\": \"Top Gainers and Losers Today\",\n",
        "        #         \"summary_text\": final_news_byte,\n",
        "        #         \"image_url\": plot_url\n",
        "        #     }\n",
        "        #     self.post_json_to_summary(json_data)\n",
        "        #     logger.info(\"JSON data posted to API successfully.\")\n",
        "        # except Exception as e:\n",
        "        #     logger.error(f\"Error posting JSON data to API: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run the class\n",
        "HighGainHighVol().run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install boto3\n",
        "# !pip uninstall -y openai\n",
        "# !pip install openai\n",
        "# !pip uninstall -y httpx\n",
        "# !pip install httpx\n",
        "# !pip install schedule"
      ],
      "metadata": {
        "id": "th_UsBOYgI1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "038fd5c1-cf3f-4c1f-936a-5694bc77867f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.35.90-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.90 (from boto3)\n",
            "  Downloading botocore-1.35.90-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.90->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.90->boto3) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.90->boto3) (1.17.0)\n",
            "Downloading boto3-1.35.90-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.90-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.35.90 botocore-1.35.90 jmespath-1.0.1 s3transfer-0.10.4\n",
            "Found existing installation: openai 1.57.4\n",
            "Uninstalling openai-1.57.4:\n",
            "  Successfully uninstalled openai-1.57.4\n",
            "Collecting openai\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-1.58.1\n",
            "Found existing installation: httpx 0.28.1\n",
            "Uninstalling httpx-0.28.1:\n",
            "  Successfully uninstalled httpx-0.28.1\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.2.2)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx\n",
            "Successfully installed httpx-0.28.1\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ]
    }
  ]
}